{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![a](res/banner.jpg)\n",
    "\n",
    "<h1 style=\"text-align: center;\">Getting into Modalities in 15mins</h1>\n",
    "\n",
    "<hr/>\n",
    "\n",
    "**Let's train a dense model with Modalities involving the following steps:**\n",
    "\n",
    "1. Data Preprocessing (Indexation, Tokenization)\n",
    "2. Model Pretraining (GPT Model)\n",
    "3. Monitoring (Weights&Biases)\n",
    "\n",
    "\n",
    "**Folder structure:**\n",
    "\n",
    "Throughout the tutorial, we will use the Jupyter Notebook `modalities_demo.ipynb` to guide us through the process. The notebook is located in the root directory of the tutorial, along with the `configs` and `data` directories. The `configs` directory contains configuration files for the model pretraining and tokenization, while the `data` directory contains subdirectories for storing checkpoints, preprocessed data, raw data, and tokenizer-related files.\n",
    "\n",
    "```text\n",
    "└── getting_started_15mins                 # Root directory for the tutorial\n",
    "    ├── modalities_demo.ipynb              # Jupyter Notebook used for the tutorial.\n",
    "    ├── configs                      \n",
    "    │   ├── pretraining_config.yaml        # Config file for the model pretraining\n",
    "    │   └── tokenization_config.yaml       # Config file for tokenization\n",
    "    └── data                         \n",
    "        ├── checkpoints                    # Dir where model and optimizer checkpoints  are stored.\n",
    "        │   └── <checkpoints>        \n",
    "        ├── preprocessed                   # Dir containing preprocessed training and eval data.\n",
    "        │   └── <files>              \n",
    "        ├── raw                      \n",
    "        │   └── fineweb_edu_num_docs_483606.jsonl   # JSONL file with raw data for training and eval.\n",
    "        └── tokenizer                \n",
    "            ├── tokenizer.json             # JSON file defining the tokenizer model.\n",
    "            └── tokenizer_config.json      # Config file specifying all tokenizer settings\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepraration steps\n",
    "\n",
    "Firstly, we need to install Modalities via pip\n",
    "\n",
    "```bash\n",
    "pip install modalities\n",
    "```\n",
    "\n",
    "and download the raw training data. \n",
    "We are going to use a  subset (500k documents) of the FineWeb-Edu dataset, as it is already cleaned, filtered and deduplicated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-08-07 11:41:02--  https://huggingface.co/datasets/ModalitiesTeam/FW_EDU_SUBSET_500k_docs/resolve/main/fineweb_edu_num_docs_483606.jsonl?download=true\n",
      "Resolving huggingface.co (huggingface.co)... 3.160.39.15, 3.160.39.99, 3.160.39.100, ...\n",
      "Connecting to huggingface.co (huggingface.co)|3.160.39.15|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://cas-bridge.xethub.hf.co/xet-bridge-us/66dacd19ae7192d89df8e0e6/14e72def15ff6a8cc24e71c9919d272c710b548b68090c3de667803d69b3d95c?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20250807%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250807T094102Z&X-Amz-Expires=3600&X-Amz-Signature=2d16da7016f41c0b4dd4dc58846f297c5c01ea6723e1ed98a62a146a13a02cba&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27fineweb_edu_num_docs_483606.jsonl%3B+filename%3D%22fineweb_edu_num_docs_483606.jsonl%22%3B&x-id=GetObject&Expires=1754563262&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1NDU2MzI2Mn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82NmRhY2QxOWFlNzE5MmQ4OWRmOGUwZTYvMTRlNzJkZWYxNWZmNmE4Y2MyNGU3MWM5OTE5ZDI3MmM3MTBiNTQ4YjY4MDkwYzNkZTY2NzgwM2Q2OWIzZDk1YyoifV19&Signature=p4S30YJqk27UGb1d4J3cbJnG3mHlsDDR-M%7Eu%7EtXSVp3qFs1oX57Xi8EPvainEjLe7VQThs%7EQmGa-ph-Yk6yctEGWJnHuuL7QfbjPE98Q8%7ELSBajUtYL5wSyPKuoQ1J3bTPzOABJd2OzG%7Ecd2PTCDXu-6Nx9xfENOd6hSdHyN-w8iCDe3cAZa8AP7cmoESNL%7EtRLbHNpBsT1Z2sXo6qlwtHdvzs3DWzRsYgRTJDn-SUJ4EoP4ofcbf0IfodousBtKW1dlHHA3x4vAh2lHdOtT8YGG3m6p2P5Fn62n%7EVJ%7EgOWVCZagolBhqvKG%7E0M3EXymj8XAscyRZWCOR3aNM5bVhg__&Key-Pair-Id=K2L8F4GPSG1IFC [following]\n",
      "--2025-08-07 11:41:02--  https://cas-bridge.xethub.hf.co/xet-bridge-us/66dacd19ae7192d89df8e0e6/14e72def15ff6a8cc24e71c9919d272c710b548b68090c3de667803d69b3d95c?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20250807%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250807T094102Z&X-Amz-Expires=3600&X-Amz-Signature=2d16da7016f41c0b4dd4dc58846f297c5c01ea6723e1ed98a62a146a13a02cba&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27fineweb_edu_num_docs_483606.jsonl%3B+filename%3D%22fineweb_edu_num_docs_483606.jsonl%22%3B&x-id=GetObject&Expires=1754563262&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1NDU2MzI2Mn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82NmRhY2QxOWFlNzE5MmQ4OWRmOGUwZTYvMTRlNzJkZWYxNWZmNmE4Y2MyNGU3MWM5OTE5ZDI3MmM3MTBiNTQ4YjY4MDkwYzNkZTY2NzgwM2Q2OWIzZDk1YyoifV19&Signature=p4S30YJqk27UGb1d4J3cbJnG3mHlsDDR-M%7Eu%7EtXSVp3qFs1oX57Xi8EPvainEjLe7VQThs%7EQmGa-ph-Yk6yctEGWJnHuuL7QfbjPE98Q8%7ELSBajUtYL5wSyPKuoQ1J3bTPzOABJd2OzG%7Ecd2PTCDXu-6Nx9xfENOd6hSdHyN-w8iCDe3cAZa8AP7cmoESNL%7EtRLbHNpBsT1Z2sXo6qlwtHdvzs3DWzRsYgRTJDn-SUJ4EoP4ofcbf0IfodousBtKW1dlHHA3x4vAh2lHdOtT8YGG3m6p2P5Fn62n%7EVJ%7EgOWVCZagolBhqvKG%7E0M3EXymj8XAscyRZWCOR3aNM5bVhg__&Key-Pair-Id=K2L8F4GPSG1IFC\n",
      "Resolving cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)... 18.64.103.68, 18.64.103.61, 18.64.103.9, ...\n",
      "Connecting to cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)|18.64.103.68|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2543881064 (2.4G)\n",
      "Saving to: ‘fineweb_edu_num_docs_483606.jsonl’\n",
      "\n",
      "fineweb_edu_num_doc 100%[===================>]   2.37G  95.4MB/s    in 27s     \n",
      "\n",
      "2025-08-07 11:41:29 (88.6 MB/s) - ‘fineweb_edu_num_docs_483606.jsonl’ saved [2543881064/2543881064]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!cd data/raw && wget https://huggingface.co/datasets/ModalitiesTeam/FW_EDU_SUBSET_500k_docs/resolve/main/fineweb_edu_num_docs_483606.jsonl?download=true -O fineweb_edu_num_docs_483606.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Disclaimer:**\n",
    "\n",
    "Don't run modalities in jupyter notebooks!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "But this time for demonstration purposes:\n",
    "\n",
    "<img src=\"res/notebooks_1.png\" alt=\"Alt text\" style=\"width:30%;\"/>\n",
    "\n",
    "<small> credits: Joel Grus - I don't like Notebooks</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before training the model, we will preprocess the raw data. In the first step, we will create an index of the data that stores the starting byte position and byte length of every document. The index will be used to efficiently index the JSONL file during the tokenization in the second step. \n",
    "\n",
    "The raw JSONL dataset and has the following properties:\n",
    "\n",
    "* Subset of FineWeb-Edu (~500k documents) encoded as JSONL file\n",
    "* already cleaned, filtered and deduplicated\n",
    "\n",
    "Each line in the JSONL is a proper JSON object containing a single document. \n",
    "```json\n",
    "{\n",
    "   \"text\":\"What is the difference between 50 Ohm and 75 Ohm Coax? [...]\",\n",
    "   \"id\":\"<urn:uuid:57e09efe-1c29-49f8-a086-e1bb5dd552c9>\",\n",
    "   \"dump\":\"CC-MAIN-2021-39\",\n",
    "   \"url\":\"http://cablesondemandblog.com/wordpress1/2014/03/\",\n",
    "   \"file_path\":\"s3://commoncrawl/crawl-data/[...]20210918002307-00380.warc.gz\",\n",
    "   \"language\":\"en\",\n",
    "   \"language_score\":0.9309850335121155,\n",
    "   \"token_count\":2355,\n",
    "   \"score\":3.625,\n",
    "   \"int_score\":4\n",
    "}\n",
    "```\n",
    "\n",
    "While the meta data is generally interesing and can be used to further filter the dataset, we are only interested in the text field for now, providing us with the actual training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of the indexation process is to determine the starting byte position and length of each document in the raw data file.\n",
    "\n",
    "Architecturally, as shown in the diagram below, a reader process reads the raw data file line by line and writes the starting byte position and length of each document to the queue. For each line in the queue, the processor first validates the JSON object and then writes the starting byte position and length of the document to the index file.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"res/modalities_indexation_bright.svg\" alt=\"Alt text\" style=\"width:80%;\"/>\n",
    "\n",
    "We run the indexation with the command shown below. \n",
    "\n",
    "The `modalities data create_raw_index` command triggers the process of creating the index from the raw data.\n",
    "The `--index_path argument` specifies the location where the generated index file will be saved. In this example, the index will be stored at `data/preprocessed/fineweb_edu_num_docs_483606.idx`.\n",
    "The last part, i.e., `data/raw/fineweb_edu_num_docs_483606.jsonl` is the input file in JSONL (JSON Lines) format containing the raw data. The command will process this file to create the index.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main - INFO - Reading raw data from data/raw/fineweb_edu_num_docs_483606.jsonl and writing index to data/preprocessed/fineweb_edu_num_docs_483606.idx ...\n",
      "main - INFO - Index file created at data/preprocessed/fineweb_edu_num_docs_483606.idx\n"
     ]
    }
   ],
   "source": [
    "!modalities data create_raw_index --index_path data/preprocessed/fineweb_edu_num_docs_483606.idx \\\n",
    "                                               data/raw/fineweb_edu_num_docs_483606.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Throughput optimized tokenization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the we have the raw JSONL dataset indexed, we can proceed with the tokenization. \n",
    "\n",
    "In Modalities, tokenization is the process of converting raw text data into a sequence of tokens that can be used as input to the model. This is achieved by scaling up the number of processors performing the tokenization on batches of documents in parallel, as shown in the diagram below. Typically, we use one processor per CPU core to maximize throughput and adapt the queue sizes and batches sizes for optimal throughput. \n",
    "\n",
    "The processors place the tokenized documents as byte streams in the queue from which the writer reads and writes the tokenized documents to the output file.\n",
    "\n",
    "<img src=\"res/modalities_tokenization_bright.svg\" alt=\"Alt text\" style=\"width:100%;\"/>\n",
    "\n",
    "The tokenized dataset file is heavily optimized for efficient indexing. As layed out in the diagram below, the header specifies the size of the data segment and size of a single token in bytes. With this information at hand, the file format is self-contained and does not need any additional information to be read. The data segment contains the concatenated byte streams of the tokenized documents.\n",
    "The documents are indexed by their starting byte position and length stored in the index segment. This allows for efficient random access to the tokenized documents in O(1) time complexity.\n",
    "\n",
    "Additionally, the shuffling of the data can be performed independently of the actual documents, as only the index can be shuffled which has a much lower memory-footprint. Internally, we implemented a numpy array-like view on top of the data segment. \n",
    "\n",
    "<img src=\"res/modalities_file_format_bright.svg\" alt=\"Alt text\" style=\"width:70%;\"/>\n",
    "\n",
    "\n",
    "We define the tokenization config as printed out below. It defines the tokenizer component including all the necessary settings to make it fully reproducible. Under settings we additionally define the performance optimization settings, such as number of CPUs to use and queue sizes, as well as, the input and output file paths.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "def display_markdown(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        code = file.read()\n",
    "    display(Markdown(f'```yaml\\n{code}\\n```'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```yaml\n",
       "settings:\n",
       "  src_path: data/raw/fineweb_edu_num_docs_483606.jsonl\n",
       "  dst_path: data/preprocessed/fineweb_edu_num_docs_483606.pbin\n",
       "  index_path: data/preprocessed/fineweb_edu_num_docs_483606.idx\n",
       "  jq_pattern: .text\n",
       "  num_cpus: ${node_env:num_cpus}\n",
       "  eod_token: <|endoftext|>\n",
       "  processing_batch_size: 10\n",
       "  raw_samples_queue_size: 300\n",
       "  processed_samples_queue_size: 300\n",
       "\n",
       "tokenizer:\n",
       "  component_key: tokenizer\n",
       "  variant_key: pretrained_hf_tokenizer\n",
       "  config:\n",
       "    pretrained_model_name_or_path: data/tokenizer\n",
       "    padding: false\n",
       "    truncation: false\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenization_config_path = \"configs/tokenization_config.yaml\"\n",
    "display_markdown(tokenization_config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instantiated <class 'modalities.tokenization.tokenizer_wrapper.PreTrainedHFTokenizer'>: tokenizer\n",
      "/home/behzad_shomali/modalities/src/modalities/tokenization/tokenizer_wrapper.py:193: UserWarning: The provided eod token <|endoftext|> has the same token id (50256) as the unk token\n",
      "  warnings.warn(f\"The provided eod token {token} has the same token id ({token_id}) as the unk token\")\n",
      "Processed batches:   0%|                             | 0/483606 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (2355 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1644 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1045 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1645 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2269 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Processed batches:   0%|                   | 70/483606 [00:00<14:03, 573.23it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1473 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1063 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1360 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1470 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1234 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1242 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Processed batches:   0%|                  | 220/483606 [00:00<08:35, 938.59it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (4300 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1106 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (15452 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1089 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1538 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Processed batches:   0%|                 | 810/483606 [00:00<02:45, 2921.19it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1063 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3021 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (4078 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1609 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2040 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1833 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1326 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1171 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1231 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Processed batches:   0%|                | 1620/483606 [00:00<02:11, 3664.50it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1140 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1067 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2575 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3397 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1333 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3766 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Processed batches:   1%|                | 3490/483606 [00:00<01:04, 7425.32it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (2344 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1173 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3158 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1158 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2749 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1158 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Processed batches:   1%|▏               | 4310/483606 [00:00<01:07, 7081.33it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1614 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2529 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1325 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1408 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2131 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1365 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1350 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Processed batches:   1%|▏               | 6190/483606 [00:00<00:52, 9177.42it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1193 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2090 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1500 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1196 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (4661 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1771 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1195 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Processed batches:   2%|▎               | 7820/483606 [00:01<00:50, 9351.25it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (4516 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1506 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2117 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3899 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2188 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1040 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1268 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1266 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1395 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1405 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1764 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (5440 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (5874 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1974 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1219 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Processed batches:   2%|▎               | 8760/483606 [00:01<01:13, 6454.94it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1840 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3457 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1451 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2274 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1184 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (5189 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1518 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2353 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1117 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (6784 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1146 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1829 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1194 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1186 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1475 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1225 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1367 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2155 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Processed batches:   2%|▎              | 11160/483606 [00:01<01:27, 5406.50it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1461 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1081 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1118 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Processed batches:   5%|▋             | 24250/483606 [00:02<00:21, 21721.73it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1025 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1936 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1449 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9587 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1554 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1108 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Processed batches:   6%|▊             | 28710/483606 [00:02<00:21, 21198.20it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1163 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3771 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (4314 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2270 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1215 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Processed batches:   7%|▉             | 32420/483606 [00:02<00:21, 21224.82it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1853 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2293 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1494 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Processed batches:   7%|█             | 35660/483606 [00:02<00:20, 22134.74it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1162 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2249 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1034 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (7646 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1761 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1304 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1365 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1319 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1433 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1269 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Processed batches:   8%|█             | 38700/483606 [00:02<00:26, 17000.11it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1085 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (4081 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1313 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1722 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2032 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2371 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Processed batches:   9%|█▏            | 41410/483606 [00:03<00:28, 15624.45it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1133 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (6452 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1109 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1360 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1082 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (6601 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1554 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1316 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1801 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1813 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2878 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Processed batches:   9%|█▎            | 44330/483606 [00:03<00:37, 11839.72it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (4880 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1304 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1348 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Processed batches:  12%|█▋            | 58070/483606 [00:03<00:15, 28009.45it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1032 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2027 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (6208 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1288 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Processed batches:  13%|█▊            | 62840/483606 [00:03<00:13, 30240.24it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1278 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1958 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2603 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1211 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1323 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1313 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1860 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1966 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1418 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1255 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1316 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Processed batches:  14%|█▉            | 67390/483606 [00:04<00:19, 21660.59it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1428 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2279 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1373 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Processed batches:  15%|██▏           | 73700/483606 [00:04<00:14, 27625.04it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1245 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1809 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1087 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Processed batches:  16%|██▎           | 78070/483606 [00:04<00:13, 29445.56it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1087 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1182 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1671 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Processed batches:  17%|██▍           | 83820/483606 [00:04<00:11, 34805.63it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1909 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2160 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1113 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1045 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9922 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1614 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2008 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (4309 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1145 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1029 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Processed batches:  18%|██▌           | 88460/483606 [00:04<00:17, 22794.69it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (4794 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (4054 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1507 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2185 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1407 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3060 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1974 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Processed batches:  20%|██▊           | 96600/483606 [00:05<00:13, 28065.66it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (3325 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2746 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1052 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2542 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1482 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1226 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (4477 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1956 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2293 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Processed batches:  21%|██▋          | 100410/483606 [00:05<00:17, 21299.87it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1031 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1259 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1289 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (5776 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1377 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1250 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2420 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2365 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Processed batches:  21%|██▊          | 103420/483606 [00:05<00:20, 18590.85it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1043 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1749 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1701 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Processed batches:  25%|███▏         | 119640/483606 [00:05<00:09, 39448.46it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1953 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1090 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Processed batches:  27%|███▍         | 129400/483606 [00:05<00:07, 49437.29it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1098 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1159 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2930 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1148 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1497 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1417 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2534 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2312 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1838 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1177 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1493 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1066 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1342 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1246 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1523 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3297 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (6812 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1724 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3041 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2779 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Processed batches:  28%|███▋         | 136950/483606 [00:06<00:12, 27318.76it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1640 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1091 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1320 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1997 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2514 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1262 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (10484 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1843 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2626 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3405 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2415 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1670 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1432 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1251 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1141 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1192 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1129 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (4342 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Processed batches:  29%|███▊         | 142610/483606 [00:07<00:18, 18295.96it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1644 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (6371 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Processed batches:  33%|████▎        | 159380/483606 [00:07<00:10, 32414.03it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1516 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1267 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8395 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2060 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1634 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1864 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Processed batches:  36%|████▋        | 172810/483606 [00:07<00:07, 42550.96it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1047 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1118 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2215 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1634 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2930 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Processed batches:  37%|████▊        | 181300/483606 [00:07<00:07, 41436.65it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1755 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1191 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1042 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1904 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1056 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1149 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1132 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1457 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (22069 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (7020 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1976 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Processed batches: 100%|█████████████| 483606/483606 [00:16<00:00, 28529.24it/s]\n"
     ]
    }
   ],
   "source": [
    "!modalities data pack_encoded_data configs/tokenization_config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Modalities, we scale up the training via Fully Sharded Data Parallel (FSDP), as defined in the paper [Zhao, Yanli, et al. \"Pytorch fsdp: experiences on scaling fully sharded data parallel.\" arXiv preprint arXiv:2304.11277 (2023).](https://arxiv.org/pdf/2304.11277)\n",
    "\n",
    "**Goal:** Maximizing the token throughput during training by trading off communication overhead for a lower memory footprint. \n",
    "\n",
    "* Before training model is split into FSDP units and each FSDP unit is sharded across all ranks\n",
    "* Each rank is a data parallel process receiving only a subset of the data\n",
    "* Each rank materializes one FSDP unit at a time during the forward pass by receving the sharded weights from its peers\n",
    "\n",
    "<img src=\"res/fsdp_bright.svg\" alt=\"Alt text\" style=\"width:90%;\"/>\n",
    "\n",
    "\n",
    "adopted from Zhao, Yanli, et al. \"Pytorch fsdp: experiences on scaling fully sharded data parallel.\" arXiv preprint arXiv:2304.11277 (2023)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While FSDP happens under the hood of Modalities the user can still parameterize the training process via the `pretraining_config.yaml` file. In fact, the training config is specified in a way that every component during training, e.g, dataset, dataloader, model, etc., are fully reproducible. On the one hand, this leads to larger, somewhat more complex config files, however it also allows to fully reproduce the training process. Especially in the field of LLMs, where the training process is expensive, complex and involves excessive amounts of ablations, this is a crucial feature to keep track of the entire configuration of the system in a reproducible manner. \n",
    "\n",
    "The config file is shown in the print out below. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```yaml\n",
       "settings:  \n",
       "  experiment_id: ${modalities_env:experiment_id}\n",
       "  config_file_path: ${modalities_env:config_file_path}\n",
       "  referencing_keys:\n",
       "    sample_key: input_ids\n",
       "    target_key: target_ids\n",
       "    prediction_key: logits\n",
       "  cuda_env:\n",
       "    local_rank: ${cuda_env:LOCAL_RANK}\n",
       "    global_rank: ${cuda_env:RANK}\n",
       "    world_size: ${cuda_env:WORLD_SIZE}\n",
       "  paths:\n",
       "    checkpoint_saving_path: data/checkpoints\n",
       "    train_dataset_path: data/preprocessed/fineweb_edu_num_docs_483606.pbin\n",
       "  intervals:\n",
       "    training_log_interval_in_steps: 5\n",
       "    checkpointing_interval_in_steps: 50\n",
       "    evaluation_interval_in_steps: 50\n",
       "  consistency_enforcement:\n",
       "    enforce_tokens_per_step_consistency: true\n",
       "    enforce_last_step_logged: false\n",
       "    enforce_last_step_evaluated: false\n",
       "    enforce_last_step_checkpointed: false\n",
       "  step_profile: \n",
       "    gradient_accumulation_steps: 1\n",
       "    local_train_micro_batch_size: 64\n",
       "    sequence_length: 256\n",
       "  training_target:\n",
       "    num_target_tokens:\n",
       "      component_key: number_conversion\n",
       "      variant_key: num_tokens_from_packed_mem_map_dataset_continuous\n",
       "      config:\n",
       "        dataset_path: ${settings.paths.train_dataset_path}\n",
       "        sequence_length: ${settings.step_profile.sequence_length}\n",
       "        num_ranks: ${settings.cuda_env.world_size}\n",
       "        local_micro_batch_size: ${settings.step_profile.local_train_micro_batch_size}\n",
       "        gradient_accumulation_steps: ${settings.step_profile.gradient_accumulation_steps}\n",
       "    num_target_steps:  # for the batch progress subscriber\n",
       "      component_key: number_conversion\n",
       "      variant_key: num_steps_from_num_tokens\n",
       "      config:\n",
       "        num_ranks: ${settings.cuda_env.world_size}\n",
       "        local_micro_batch_size: ${settings.step_profile.local_train_micro_batch_size}\n",
       "        global_num_tokens: ${settings.training_target.num_target_tokens}\n",
       "        sequence_length: ${settings.step_profile.sequence_length}\n",
       "        gradient_accumulation_steps: ${settings.step_profile.gradient_accumulation_steps}\n",
       "  training_progress: \n",
       "    global_num_seen_tokens: 0\n",
       "    num_seen_steps: 0\n",
       "    num_seen_samples: 0\n",
       "    last_step: -1\n",
       "\n",
       "collate_fn:\n",
       "  component_key: collate_fn\n",
       "  variant_key: gpt_2_llm_collator\n",
       "  config:\n",
       "    sample_key: ${settings.referencing_keys.sample_key}\n",
       "    target_key: ${settings.referencing_keys.target_key}\n",
       "\n",
       "train_dataset:\n",
       "  component_key: dataset\n",
       "  variant_key: packed_mem_map_dataset_continuous\n",
       "  config:\n",
       "    raw_data_path: ${settings.paths.train_dataset_path}\n",
       "    sequence_length: ${settings.step_profile.sequence_length}\n",
       "    sample_key:  ${settings.referencing_keys.sample_key}\n",
       "\n",
       "train_dataloader:\n",
       "  component_key: data_loader\n",
       "  variant_key: default\n",
       "  config:\n",
       "    num_workers: 2\n",
       "    pin_memory: true\n",
       "    dataloader_tag: train\n",
       "    dataset:\n",
       "      instance_key: train_dataset\n",
       "      pass_type: BY_REFERENCE\n",
       "    batch_sampler:\n",
       "      component_key: batch_sampler\n",
       "      variant_key: default\n",
       "      config:\n",
       "        batch_size: ${settings.step_profile.local_train_micro_batch_size}\n",
       "        drop_last: true\n",
       "        sampler:\n",
       "          component_key: sampler\n",
       "          variant_key: resumable_distributed_sampler\n",
       "          config:\n",
       "            dataset:\n",
       "              instance_key: train_dataset\n",
       "              pass_type: BY_REFERENCE\n",
       "            rank: ${settings.cuda_env.global_rank}\n",
       "            num_replicas: ${settings.cuda_env.world_size}\n",
       "            shuffle: true\n",
       "            seed: 42\n",
       "            drop_last: true\n",
       "            skip_num_global_samples: ${settings.training_progress.num_seen_samples}\n",
       "    collate_fn:\n",
       "      instance_key: collate_fn\n",
       "      pass_type: BY_REFERENCE\n",
       "\n",
       "eval_dataloaders: []\n",
       "\n",
       "checkpoint_saving:\n",
       "  component_key: checkpoint_saving\n",
       "  variant_key: default\n",
       "  config:\n",
       "    checkpoint_saving_strategy:\n",
       "      component_key: checkpoint_saving_strategy\n",
       "      variant_key: save_k_most_recent_checkpoints_strategy\n",
       "      config:\n",
       "        k: -1   # -1 to save all checkpoints\n",
       "    checkpoint_saving_execution:\n",
       "      component_key: checkpoint_saving_execution\n",
       "      variant_key: fsdp1\n",
       "      config:\n",
       "        checkpoint_path: ${settings.paths.checkpoint_saving_path}\n",
       "        global_rank: ${settings.cuda_env.global_rank}\n",
       "        experiment_id: ${settings.experiment_id}\n",
       "\n",
       "loss_fn:\n",
       "  component_key: loss\n",
       "  variant_key: clm_cross_entropy_loss\n",
       "  config:\n",
       "    target_key: ${settings.referencing_keys.target_key}\n",
       "    prediction_key: ${settings.referencing_keys.prediction_key}\n",
       "\n",
       "wrapped_model:\n",
       "  component_key: model\n",
       "  variant_key: fsdp_wrapped\n",
       "  config:\n",
       "    model:\n",
       "      instance_key: model\n",
       "      pass_type: BY_REFERENCE\n",
       "    sync_module_states: true\n",
       "    mixed_precision_settings: BF_16\n",
       "    sharding_strategy: FULL_SHARD\n",
       "    block_names: [GPT2Block]\n",
       "\n",
       "model:\n",
       "  component_key: model\n",
       "  variant_key: model_initialized\n",
       "  config:\n",
       "    model:\n",
       "      instance_key: model_raw\n",
       "      pass_type: BY_REFERENCE\n",
       "    model_initializer:\n",
       "      component_key: model_initialization\n",
       "      variant_key: composed\n",
       "      config:\n",
       "        model_type: gpt2\n",
       "        weight_init_type: scaled\n",
       "        mean: 0.0\n",
       "        std: 0.02\n",
       "        num_layers: ${model_raw.config.n_layer}\n",
       "\n",
       "model_raw:\n",
       "  component_key: model\n",
       "  variant_key: gpt2\n",
       "  config:\n",
       "    sample_key: ${settings.referencing_keys.sample_key}\n",
       "    poe_type: NOPE\n",
       "    sequence_length: ${settings.step_profile.sequence_length}\n",
       "    prediction_key: ${loss_fn.config.prediction_key}\n",
       "    vocab_size: 50304 # GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency\n",
       "    n_layer: 2\n",
       "    n_head_q: 8\n",
       "    n_head_kv: 4\n",
       "    ffn_hidden: 128\n",
       "    n_embd: 128\n",
       "    dropout: 0.0\n",
       "    bias: false\n",
       "    attention_config:\n",
       "      qkv_transforms:\n",
       "      - type_hint: RotaryTransform\n",
       "        config:\n",
       "          n_embd: ${model_raw.config.n_embd}\n",
       "          n_head: ${model_raw.config.n_head_q}\n",
       "          seq_length_dim: -2\n",
       "          base_freq: 100000\n",
       "    attention_implementation: pytorch_flash\n",
       "    activation_type: swiglu\n",
       "    attention_norm_config:\n",
       "      norm_type: rms_norm\n",
       "      config:\n",
       "        ndim: ${model_raw.config.n_embd}\n",
       "        bias: true\n",
       "        epsilon: 1e-5\n",
       "    ffn_norm_config:\n",
       "      norm_type: rms_norm\n",
       "      config:\n",
       "        ndim: ${model_raw.config.n_embd}\n",
       "        bias: true\n",
       "        epsilon: 1e-5\n",
       "    lm_head_norm_config:\n",
       "      norm_type: rms_norm\n",
       "      config:\n",
       "        ndim: ${model_raw.config.n_embd}\n",
       "        bias: true\n",
       "        epsilon: 1e-5\n",
       "\n",
       "scheduler:\n",
       "  component_key: scheduler\n",
       "  variant_key: onecycle_lr\n",
       "  config:\n",
       "    optimizer:\n",
       "      instance_key: optimizer\n",
       "      pass_type: BY_REFERENCE\n",
       "    max_lr: 6e-4\n",
       "    div_factor: 10\n",
       "    final_div_factor: 1\n",
       "    total_steps: ${settings.training_target.num_target_steps}\n",
       "    pct_start: 0.01\n",
       "    anneal_strategy: cos\n",
       "    last_epoch: ${settings.training_progress.last_step}\n",
       "\n",
       "optimizer:\n",
       "  component_key: optimizer\n",
       "  variant_key: adam_w\n",
       "  config:\n",
       "    lr: 0.0001\n",
       "    betas: [0.9, 0.95]\n",
       "    eps: 1e-8\n",
       "    weight_decay: 1e-1\n",
       "    weight_decay_groups_excluded: [embedding, layernorm]\n",
       "    wrapped_model: \n",
       "      instance_key: wrapped_model\n",
       "      pass_type: BY_REFERENCE\n",
       "\n",
       "gradient_clipper:\n",
       "  component_key: gradient_clipper\n",
       "  variant_key: fsdp\n",
       "  config:\n",
       "    wrapped_model:\n",
       "      instance_key: wrapped_model\n",
       "      pass_type: BY_REFERENCE\n",
       "    norm_type: P2_NORM\n",
       "    max_norm: 1.0\n",
       "\n",
       "progress_subscriber:\n",
       "  component_key: progress_subscriber\n",
       "  variant_key: dummy\n",
       "  config: {}\n",
       "\n",
       "evaluation_subscriber:\n",
       "  component_key: results_subscriber\n",
       "  variant_key: wandb\n",
       "  config:\n",
       "    global_rank: ${settings.cuda_env.global_rank}\n",
       "    project: ai_24_demo\n",
       "    mode: OFFLINE\n",
       "    experiment_id: ${settings.experiment_id}\n",
       "    directory: wandb_storage\n",
       "    config_file_path: ${settings.config_file_path}\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenization_config_path = \"configs/pretraining_config.yaml\"\n",
    "display_markdown(tokenization_config_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below you find the command for running the distributed training with modalities across multiple 4 GPUs on a single node. Let's break it down into its components:\n",
    "\n",
    "* `CUDA_VISIBLE_DEVICES=0,1,2,3`: This environment variable specifies which GPUs will be used for the job. In this case, GPUs with IDs 0, 1, 2, 3 are selected for training.\n",
    "\n",
    "* `torchrun`: This is a utility from PyTorch used to launch distributed training. It automatically manages multiple processes for distributed training.\n",
    "\n",
    "* `--rdzv-endpoint localhost:29515`: Specifies the rendezvous endpoint. Here, localhost is the machine's address, and 29515 is the port. The rendezvous endpoint coordinates the processes involved in distributed training.\n",
    "\n",
    "* `--nnodes 1`: Specifies the number of nodes to be used in the distributed setup. Since this is a single-node setup, 1 is used.\n",
    "\n",
    "* `--nproc_per_node 4`: This argument tells torchrun how many processes to launch on each node. In this case, 4 processes are launched per node, corresponding to the 4 GPUs (IDs 0, 1, 2, 3) specified by `CUDA_VISIBLE_DEVICES`.\n",
    "\n",
    "* `$(which modalities) run`: This part dynamically finds the path to the modalities executable and runs it. The run command triggers the main process to start the training.\n",
    "\n",
    "* `--config_file_path configs/pretraining_config.yaml`: The `--config_file_path` argument provides the path to the configuration file for the training job. In this example, the configuration is provided in `configs/pretraining_config.yaml`, which includes settings like model architecture, optimizer, dataset, dataloader and other training components.\n",
    "\n",
    "\n",
    "Once executed, the training process will start, and you will see the training logs in the terminal. The logs will include information about the training progress, such as the loss values, learning rate, and other metrics. Additionally, you can monitor the training process using Weights & Biases, which modalities automatically logs. Make sure that you are logged into your Weights & Biases account to track the training metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W0807 11:43:40.118000 3516751 site-packages/torch/distributed/run.py:792] \n",
      "W0807 11:43:40.118000 3516751 site-packages/torch/distributed/run.py:792] *****************************************\n",
      "W0807 11:43:40.118000 3516751 site-packages/torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W0807 11:43:40.118000 3516751 site-packages/torch/distributed/run.py:792] *****************************************\n",
      "libibverbs: Warning: couldn't load driver 'libvmw_pvrdma-rdmav34.so': libvmw_pvrdma-rdmav34.so: cannot open shared object file: No such file or directory\n",
      "libibverbs: Warning: couldn't load driver 'libvmw_pvrdma-rdmav34.so': libvmw_pvrdma-rdmav34.so: cannot open shared object file: No such file or directory\n",
      "libibverbs: Warning: couldn't load driver 'libvmw_pvrdma-rdmav34.so': libvmw_pvrdma-rdmav34.so: cannot open shared object file: No such file or directory\n",
      "libibverbs: Warning: couldn't load driver 'libvmw_pvrdma-rdmav34.so': libvmw_pvrdma-rdmav34.so: cannot open shared object file: No such file or directory\n",
      "Rank 0 received experiment_id: 2025-08-07__11-43-45_7d9fc15e\n",
      "Rank 2 received experiment_id: 2025-08-07__11-43-45_7d9fc15e\n",
      "Rank 3 received experiment_id: 2025-08-07__11-43-45_7d9fc15e\n",
      "Rank 1 received experiment_id: 2025-08-07__11-43-45_7d9fc15e\n",
      "[rank0]: Traceback (most recent call last):\n",
      "[rank0]:   File \"/raid/s3/opengptx/behzad_shomali/miniforge3/envs/modalities/bin/modalities\", line 7, in <module>\n",
      "[rank0]:     sys.exit(main())\n",
      "[rank0]:              ^^^^^^\n",
      "[rank0]:   File \"/raid/s3/opengptx/behzad_shomali/miniforge3/envs/modalities/lib/python3.11/site-packages/click/core.py\", line 1442, in __call__\n",
      "[rank0]:     return self.main(*args, **kwargs)\n",
      "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/raid/s3/opengptx/behzad_shomali/miniforge3/envs/modalities/lib/python3.11/site-packages/click/core.py\", line 1363, in main\n",
      "[rank0]:     rv = self.invoke(ctx)\n",
      "[rank0]:          ^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/raid/s3/opengptx/behzad_shomali/miniforge3/envs/modalities/lib/python3.11/site-packages/click/core.py\", line 1830, in invoke\n",
      "[rank0]:     return _process_result(sub_ctx.command.invoke(sub_ctx))\n",
      "[rank0]:                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/raid/s3/opengptx/behzad_shomali/miniforge3/envs/modalities/lib/python3.11/site-packages/click/core.py\", line 1226, in invoke\n",
      "[rank0]:     return ctx.invoke(self.callback, **ctx.params)\n",
      "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/raid/s3/opengptx/behzad_shomali/miniforge3/envs/modalities/lib/python3.11/site-packages/click/core.py\", line 794, in invoke\n",
      "[rank0]:     return callback(*args, **kwargs)\n",
      "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/home/behzad_shomali/modalities/src/modalities/__main__.py\", line 66, in CMD_entry_point_run_modalities\n",
      "[rank0]:     components = main_obj.build_components(components_model_type=TrainingComponentsInstantiationModel)\n",
      "[rank0]:                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/home/behzad_shomali/modalities/src/modalities/main.py\", line 82, in build_components\n",
      "[rank0]:     components = self.component_factory.build_components(\n",
      "[rank0]:                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/home/behzad_shomali/modalities/src/modalities/config/component_factory.py\", line 42, in build_components\n",
      "[rank0]:     component_dict = self._build_config(\n",
      "[rank0]:                      ^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/home/behzad_shomali/modalities/src/modalities/config/component_factory.py\", line 53, in _build_config\n",
      "[rank0]:     component_dict_filtered = {name: config_dict[name] for name in component_names_required}\n",
      "[rank0]:                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/home/behzad_shomali/modalities/src/modalities/config/component_factory.py\", line 53, in <dictcomp>\n",
      "[rank0]:     component_dict_filtered = {name: config_dict[name] for name in component_names_required}\n",
      "[rank0]:                                      ~~~~~~~~~~~^^^^^^\n",
      "[rank0]: KeyError: 'app_state'\n",
      "[rank3]: Traceback (most recent call last):\n",
      "[rank3]:   File \"/raid/s3/opengptx/behzad_shomali/miniforge3/envs/modalities/bin/modalities\", line 7, in <module>\n",
      "[rank3]:     sys.exit(main())\n",
      "[rank3]:              ^^^^^^\n",
      "[rank3]:   File \"/raid/s3/opengptx/behzad_shomali/miniforge3/envs/modalities/lib/python3.11/site-packages/click/core.py\", line 1442, in __call__\n",
      "[rank3]:     return self.main(*args, **kwargs)\n",
      "[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank3]:   File \"/raid/s3/opengptx/behzad_shomali/miniforge3/envs/modalities/lib/python3.11/site-packages/click/core.py\", line 1363, in main\n",
      "[rank3]:     rv = self.invoke(ctx)\n",
      "[rank3]:          ^^^^^^^^^^^^^^^^\n",
      "[rank3]:   File \"/raid/s3/opengptx/behzad_shomali/miniforge3/envs/modalities/lib/python3.11/site-packages/click/core.py\", line 1830, in invoke\n",
      "[rank3]:     return _process_result(sub_ctx.command.invoke(sub_ctx))\n",
      "[rank3]:                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank3]:   File \"/raid/s3/opengptx/behzad_shomali/miniforge3/envs/modalities/lib/python3.11/site-packages/click/core.py\", line 1226, in invoke\n",
      "[rank3]:     return ctx.invoke(self.callback, **ctx.params)\n",
      "[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank3]:   File \"/raid/s3/opengptx/behzad_shomali/miniforge3/envs/modalities/lib/python3.11/site-packages/click/core.py\", line 794, in invoke\n",
      "[rank3]:     return callback(*args, **kwargs)\n",
      "[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank3]:   File \"/home/behzad_shomali/modalities/src/modalities/__main__.py\", line 66, in CMD_entry_point_run_modalities\n",
      "[rank3]:     components = main_obj.build_components(components_model_type=TrainingComponentsInstantiationModel)\n",
      "[rank3]:                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank3]:   File \"/home/behzad_shomali/modalities/src/modalities/main.py\", line 82, in build_components\n",
      "[rank3]:     components = self.component_factory.build_components(\n",
      "[rank3]:                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank3]:   File \"/home/behzad_shomali/modalities/src/modalities/config/component_factory.py\", line 42, in build_components\n",
      "[rank3]:     component_dict = self._build_config(\n",
      "[rank3]:                      ^^^^^^^^^^^^^^^^^^^\n",
      "[rank3]:   File \"/home/behzad_shomali/modalities/src/modalities/config/component_factory.py\", line 53, in _build_config\n",
      "[rank3]:     component_dict_filtered = {name: config_dict[name] for name in component_names_required}\n",
      "[rank3]:                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank3]:   File \"/home/behzad_shomali/modalities/src/modalities/config/component_factory.py\", line 53, in <dictcomp>\n",
      "[rank3]:     component_dict_filtered = {name: config_dict[name] for name in component_names_required}\n",
      "[rank3]:                                      ~~~~~~~~~~~^^^^^^\n",
      "[rank3]: KeyError: 'app_state'\n",
      "[rank2]: Traceback (most recent call last):\n",
      "[rank2]:   File \"/raid/s3/opengptx/behzad_shomali/miniforge3/envs/modalities/bin/modalities\", line 7, in <module>\n",
      "[rank2]:     sys.exit(main())\n",
      "[rank2]:              ^^^^^^\n",
      "[rank2]:   File \"/raid/s3/opengptx/behzad_shomali/miniforge3/envs/modalities/lib/python3.11/site-packages/click/core.py\", line 1442, in __call__\n",
      "[rank2]:     return self.main(*args, **kwargs)\n",
      "[rank2]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank2]:   File \"/raid/s3/opengptx/behzad_shomali/miniforge3/envs/modalities/lib/python3.11/site-packages/click/core.py\", line 1363, in main\n",
      "[rank2]:     rv = self.invoke(ctx)\n",
      "[rank2]:          ^^^^^^^^^^^^^^^^\n",
      "[rank2]:   File \"/raid/s3/opengptx/behzad_shomali/miniforge3/envs/modalities/lib/python3.11/site-packages/click/core.py\", line 1830, in invoke\n",
      "[rank2]:     return _process_result(sub_ctx.command.invoke(sub_ctx))\n",
      "[rank2]:                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank2]:   File \"/raid/s3/opengptx/behzad_shomali/miniforge3/envs/modalities/lib/python3.11/site-packages/click/core.py\", line 1226, in invoke\n",
      "[rank2]:     return ctx.invoke(self.callback, **ctx.params)\n",
      "[rank2]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank2]:   File \"/raid/s3/opengptx/behzad_shomali/miniforge3/envs/modalities/lib/python3.11/site-packages/click/core.py\", line 794, in invoke\n",
      "[rank2]:     return callback(*args, **kwargs)\n",
      "[rank2]:            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank2]:   File \"/home/behzad_shomali/modalities/src/modalities/__main__.py\", line 66, in CMD_entry_point_run_modalities\n",
      "[rank2]:     components = main_obj.build_components(components_model_type=TrainingComponentsInstantiationModel)\n",
      "[rank2]:                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank2]:   File \"/home/behzad_shomali/modalities/src/modalities/main.py\", line 82, in build_components\n",
      "[rank2]:     components = self.component_factory.build_components(\n",
      "[rank2]:                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank2]:   File \"/home/behzad_shomali/modalities/src/modalities/config/component_factory.py\", line 42, in build_components\n",
      "[rank2]:     component_dict = self._build_config(\n",
      "[rank2]:                      ^^^^^^^^^^^^^^^^^^^\n",
      "[rank2]:   File \"/home/behzad_shomali/modalities/src/modalities/config/component_factory.py\", line 53, in _build_config\n",
      "[rank2]:     component_dict_filtered = {name: config_dict[name] for name in component_names_required}\n",
      "[rank2]:                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank2]:   File \"/home/behzad_shomali/modalities/src/modalities/config/component_factory.py\", line 53, in <dictcomp>\n",
      "[rank2]:     component_dict_filtered = {name: config_dict[name] for name in component_names_required}\n",
      "[rank2]:                                      ~~~~~~~~~~~^^^^^^\n",
      "[rank2]: KeyError: 'app_state'\n",
      "[rank1]: Traceback (most recent call last):\n",
      "[rank1]:   File \"/raid/s3/opengptx/behzad_shomali/miniforge3/envs/modalities/bin/modalities\", line 7, in <module>\n",
      "[rank1]:     sys.exit(main())\n",
      "[rank1]:              ^^^^^^\n",
      "[rank1]:   File \"/raid/s3/opengptx/behzad_shomali/miniforge3/envs/modalities/lib/python3.11/site-packages/click/core.py\", line 1442, in __call__\n",
      "[rank1]:     return self.main(*args, **kwargs)\n",
      "[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank1]:   File \"/raid/s3/opengptx/behzad_shomali/miniforge3/envs/modalities/lib/python3.11/site-packages/click/core.py\", line 1363, in main\n",
      "[rank1]:     rv = self.invoke(ctx)\n",
      "[rank1]:          ^^^^^^^^^^^^^^^^\n",
      "[rank1]:   File \"/raid/s3/opengptx/behzad_shomali/miniforge3/envs/modalities/lib/python3.11/site-packages/click/core.py\", line 1830, in invoke\n",
      "[rank1]:     return _process_result(sub_ctx.command.invoke(sub_ctx))\n",
      "[rank1]:                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank1]:   File \"/raid/s3/opengptx/behzad_shomali/miniforge3/envs/modalities/lib/python3.11/site-packages/click/core.py\", line 1226, in invoke\n",
      "[rank1]:     return ctx.invoke(self.callback, **ctx.params)\n",
      "[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank1]:   File \"/raid/s3/opengptx/behzad_shomali/miniforge3/envs/modalities/lib/python3.11/site-packages/click/core.py\", line 794, in invoke\n",
      "[rank1]:     return callback(*args, **kwargs)\n",
      "[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank1]:   File \"/home/behzad_shomali/modalities/src/modalities/__main__.py\", line 66, in CMD_entry_point_run_modalities\n",
      "[rank1]:     components = main_obj.build_components(components_model_type=TrainingComponentsInstantiationModel)\n",
      "[rank1]:                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank1]:   File \"/home/behzad_shomali/modalities/src/modalities/main.py\", line 82, in build_components\n",
      "[rank1]:     components = self.component_factory.build_components(\n",
      "[rank1]:                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank1]:   File \"/home/behzad_shomali/modalities/src/modalities/config/component_factory.py\", line 42, in build_components\n",
      "[rank1]:     component_dict = self._build_config(\n",
      "[rank1]:                      ^^^^^^^^^^^^^^^^^^^\n",
      "[rank1]:   File \"/home/behzad_shomali/modalities/src/modalities/config/component_factory.py\", line 53, in _build_config\n",
      "[rank1]:     component_dict_filtered = {name: config_dict[name] for name in component_names_required}\n",
      "[rank1]:                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank1]:   File \"/home/behzad_shomali/modalities/src/modalities/config/component_factory.py\", line 53, in <dictcomp>\n",
      "[rank1]:     component_dict_filtered = {name: config_dict[name] for name in component_names_required}\n",
      "[rank1]:                                      ~~~~~~~~~~~^^^^^^\n",
      "[rank1]: KeyError: 'app_state'\n",
      "W0807 11:43:50.342000 3516751 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 3516958 closing signal SIGTERM\n",
      "W0807 11:43:50.343000 3516751 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 3516961 closing signal SIGTERM\n",
      "W0807 11:43:50.343000 3516751 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 3516962 closing signal SIGTERM\n",
      "E0807 11:43:51.024000 3516751 site-packages/torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 1) local_rank: 0 (pid: 3516957) of binary: /raid/s3/opengptx/behzad_shomali/miniforge3/envs/modalities/bin/python3.11\n",
      "Traceback (most recent call last):\n",
      "  File \"/raid/s3/opengptx/behzad_shomali/miniforge3/envs/modalities/bin/torchrun\", line 7, in <module>\n",
      "    sys.exit(main())\n",
      "             ^^^^^^\n",
      "  File \"/raid/s3/opengptx/behzad_shomali/miniforge3/envs/modalities/lib/python3.11/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 355, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/raid/s3/opengptx/behzad_shomali/miniforge3/envs/modalities/lib/python3.11/site-packages/torch/distributed/run.py\", line 918, in main\n",
      "    run(args)\n",
      "  File \"/raid/s3/opengptx/behzad_shomali/miniforge3/envs/modalities/lib/python3.11/site-packages/torch/distributed/run.py\", line 909, in run\n",
      "    elastic_launch(\n",
      "  File \"/raid/s3/opengptx/behzad_shomali/miniforge3/envs/modalities/lib/python3.11/site-packages/torch/distributed/launcher/api.py\", line 138, in __call__\n",
      "    return launch_agent(self._config, self._entrypoint, list(args))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/raid/s3/opengptx/behzad_shomali/miniforge3/envs/modalities/lib/python3.11/site-packages/torch/distributed/launcher/api.py\", line 269, in launch_agent\n",
      "    raise ChildFailedError(\n",
      "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
      "============================================================\n",
      "/raid/s3/opengptx/behzad_shomali/miniforge3/envs/modalities/bin/modalities FAILED\n",
      "------------------------------------------------------------\n",
      "Failures:\n",
      "  <NO_OTHER_FAILURES>\n",
      "------------------------------------------------------------\n",
      "Root Cause (first observed failure):\n",
      "[0]:\n",
      "  time      : 2025-08-07_11:43:50\n",
      "  host      : dgx2\n",
      "  rank      : 0 (local_rank: 0)\n",
      "  exitcode  : 1 (pid: 3516957)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "! CUDA_VISIBLE_DEVICES=0,1,2,3 torchrun --rdzv-endpoint localhost:29515 \\\n",
    "                                        --nnodes 1 \\\n",
    "                                        --nproc_per_node 4 \\\n",
    "                                        $(which modalities) run --config_file_path configs/pretraining_config.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "modalities",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
