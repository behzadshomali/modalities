{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c829b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from modalities.tokenization.tokenizer_wrapper import TokenizerWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447960b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextInferenceComponent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        tokenizer: TokenizerWrapper,\n",
    "        prompt_template: str,\n",
    "        sequence_length: int,\n",
    "        temperature: float,\n",
    "        eod_token: str,\n",
    "        device: torch.device,\n",
    "    ) -> None:\n",
    "        self.model = model\n",
    "        self.model.to(device)\n",
    "        self.model.eval()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.eod_token = eod_token\n",
    "        self.prompt_template = prompt_template\n",
    "        self.temperature = temperature\n",
    "        self.sequence_length = sequence_length\n",
    "        self.device = device\n",
    "\n",
    "    def generate_tokens(\n",
    "        self,\n",
    "        context: str,\n",
    "    ):\n",
    "        token_ids_list = self.tokenizer.tokenize(context)\n",
    "        max_new_tokens = self.sequence_length - len(token_ids_list)\n",
    "        input_token_ids = torch.IntTensor(token_ids_list).to(self.device).unsqueeze(0)\n",
    "        input_dict = {\"input_ids\": input_token_ids}\n",
    "\n",
    "        print(\"--------------------PROMPT--------------------\")\n",
    "        context_decoded = self.tokenizer.decode(token_ids_list)\n",
    "        print(\"Prompt: \", context_decoded, end=\"\")\n",
    "\n",
    "        print(\"\\n\\n--------------------OUTPUT--------------------\\n\")\n",
    "        generated_token_ids = []\n",
    "        generated_text_old = \"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits = self.model(input_dict)[\"logits\"]\n",
    "            logits = logits[:, -1, :]\n",
    "            if self.temperature > 0:\n",
    "                logits = logits / self.temperature\n",
    "                probs = F.softmax(logits, dim=-1)\n",
    "                idx_next = torch.multinomial(probs, num_samples=1)\n",
    "                token_id: int = idx_next[0, 0].item()\n",
    "            else:\n",
    "                idx_next = torch.argmax(logits, dim=-1)\n",
    "                token_id: int = idx_next.item()\n",
    "            generated_token_ids.append(token_id)\n",
    "            idx_next_str = self.tokenizer.decode([token_id])\n",
    "            generated_text_new = self.tokenizer.decode(generated_token_ids)\n",
    "\n",
    "            if idx_next_str == self.eod_token:\n",
    "                print(\"\\n<reached end of document token>\", end=\"\")\n",
    "                break\n",
    "            else:\n",
    "                diff_text = generated_text_new[len(generated_text_old) :]\n",
    "                generated_text_old = generated_text_new\n",
    "                print(diff_text, end=\"\")\n",
    "                sys.stdout.flush()\n",
    "                token_ids_list.append(token_id)\n",
    "                input_token_ids = torch.IntTensor(token_ids_list).to(self.device).unsqueeze(0)\n",
    "                input_dict = {\"input_ids\": input_token_ids}\n",
    "        print(\"\\n max tokens reached\", end=\"\")\n",
    "\n",
    "    def run(self):\n",
    "        prompt = TextInferenceComponent._get_prompt(self.prompt_template)\n",
    "        try:\n",
    "            self.generate_tokens(context=prompt)\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"closing app...\")\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_prompt(template: str) -> str:\n",
    "        # Regular expression to find {variable_name}\n",
    "        pattern = re.compile(r\"\\{(.*?)\\}\")\n",
    "\n",
    "        # Find all occurrences of the pattern\n",
    "        variable_names = pattern.findall(template)\n",
    "\n",
    "        # Dictionary to hold variable names and user provided values\n",
    "        user_inputs = {}\n",
    "\n",
    "        # Ask user for input for each found variable name\n",
    "        for var in variable_names:\n",
    "            user_inputs[var] = input(f\"{var}: \")\n",
    "\n",
    "        # Use str.format() to replace placeholders with user values\n",
    "        formatted_string = template.format(**user_inputs)\n",
    "        if len(formatted_string) == 0:\n",
    "            raise ValueError(\"Prompt is empty\")\n",
    "        return formatted_string\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "modalities_orig",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
