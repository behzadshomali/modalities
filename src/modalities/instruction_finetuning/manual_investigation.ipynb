{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b5dc3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4,5\"\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "from trl import SFTTrainer, SFTConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25dda3e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.71it/s]\n"
     ]
    }
   ],
   "source": [
    "model_name = \"Behzadshomali/Teuken3.7B\"\n",
    "device = \"cuda:0\"\n",
    "\n",
    "base_tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c70b339",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 8/8 [00:00<00:00, 327.45it/s]\n"
     ]
    }
   ],
   "source": [
    "sft_model_name = \"Behzadshomali/Teuken3.7B_IT_LoRA\"\n",
    "revision = \"22_04_11_checkpoint-1125\"\n",
    "\n",
    "sft_tokenizer = AutoTokenizer.from_pretrained(sft_model_name, trust_remote_code=True, revision=revision)\n",
    "sft_model = AutoModelForCausalLM.from_pretrained(sft_model_name, trust_remote_code=True, revision=revision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd37408c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "982cde06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "The model 'GPT2ForCausalLM' is not supported for text-generation. Supported models are ['PeftModelForCausalLM', 'ArceeForCausalLM', 'AriaTextForCausalLM', 'BambaForCausalLM', 'BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BitNetForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'Cohere2ForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'DeepseekV2ForCausalLM', 'DeepseekV3ForCausalLM', 'DiffLlamaForCausalLM', 'DogeForCausalLM', 'Dots1ForCausalLM', 'ElectraForCausalLM', 'Emu3ForCausalLM', 'ErnieForCausalLM', 'Ernie4_5ForCausalLM', 'Ernie4_5_MoeForCausalLM', 'Exaone4ForCausalLM', 'FalconForCausalLM', 'FalconH1ForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'Gemma3ForConditionalGeneration', 'Gemma3ForCausalLM', 'Gemma3nForConditionalGeneration', 'Gemma3nForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'Glm4ForCausalLM', 'Glm4MoeForCausalLM', 'GotOcr2ForConditionalGeneration', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GptOssForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'GraniteMoeHybridForCausalLM', 'GraniteMoeSharedForCausalLM', 'HeliumForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'Lfm2ForCausalLM', 'LlamaForCausalLM', 'Llama4ForCausalLM', 'Llama4ForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MiniMaxForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'ModernBertDecoderForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'Olmo2ForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'Phi4MultimodalForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen3ForCausalLM', 'Qwen3MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'SmolLM3ForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'xLSTMForCausalLM', 'XmodForCausalLM', 'ZambaForCausalLM', 'Zamba2ForCausalLM'].\n",
      "Device set to use cuda:0\n",
      "The model 'GPT2ForCausalLM' is not supported for text-generation. Supported models are ['PeftModelForCausalLM', 'ArceeForCausalLM', 'AriaTextForCausalLM', 'BambaForCausalLM', 'BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BitNetForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'Cohere2ForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'DeepseekV2ForCausalLM', 'DeepseekV3ForCausalLM', 'DiffLlamaForCausalLM', 'DogeForCausalLM', 'Dots1ForCausalLM', 'ElectraForCausalLM', 'Emu3ForCausalLM', 'ErnieForCausalLM', 'Ernie4_5ForCausalLM', 'Ernie4_5_MoeForCausalLM', 'Exaone4ForCausalLM', 'FalconForCausalLM', 'FalconH1ForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'Gemma3ForConditionalGeneration', 'Gemma3ForCausalLM', 'Gemma3nForConditionalGeneration', 'Gemma3nForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'Glm4ForCausalLM', 'Glm4MoeForCausalLM', 'GotOcr2ForConditionalGeneration', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GptOssForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'GraniteMoeHybridForCausalLM', 'GraniteMoeSharedForCausalLM', 'HeliumForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'Lfm2ForCausalLM', 'LlamaForCausalLM', 'Llama4ForCausalLM', 'Llama4ForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MiniMaxForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'ModernBertDecoderForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'Olmo2ForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'Phi4MultimodalForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen3ForCausalLM', 'Qwen3MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'SmolLM3ForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'xLSTMForCausalLM', 'XmodForCausalLM', 'ZambaForCausalLM', 'Zamba2ForCausalLM'].\n"
     ]
    }
   ],
   "source": [
    "# question = \"James decides to run 3 sprints 3 times a week. He runs 60 meters each sprint. How many total meters does he run a week?\"\n",
    "generator_base = pipeline(\n",
    "    \"text-generation\", \n",
    "    base_model, \n",
    "    tokenizer=base_tokenizer, \n",
    "    device_map=\"auto\",\n",
    "    temperature=.7\n",
    ")\n",
    "\n",
    "generator_sft = pipeline(\n",
    "    \"text-generation\", \n",
    "    sft_model, \n",
    "    tokenizer=sft_tokenizer, \n",
    "    device_map=\"auto\",\n",
    "    temperature=.3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d13fe442",
   "metadata": {},
   "outputs": [],
   "source": [
    "question2 = \"Marissa is hiking a 12-mile trail. She took 1 hour to walk the first 4 miles, then another hour to walk the next two miles. If she wants her average speed to be 4 miles per hour, what speed (in miles per hour) does she need to walk the remaining distance?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "03b71273",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_base = generator_base([question2], max_new_tokens=512, return_full_text=False)[0]\n",
    "output_sft = generator_sft([{\"role\": \"user\", \"content\": question2}], max_new_tokens=512, return_full_text=False)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72c6c27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE:\n",
      " \n",
      "\n",
      "Marissa 12 miles of trail in 2 hours. If she wants her average speed to be 4 miles per hour, what speed (in miles per hour) does she need to walk the remaining distance?\n",
      "\n",
      "What is the answer?\n",
      "\n",
      "I am stuck on this question. Can someone please help me?\n",
      "\n",
      "👍\n",
      "\n",
      "👎\n",
      "\n",
      "ℹ️\n",
      "\n",
      "🚩\n",
      "\n",
      "Anonymous\n",
      "\n",
      "Aug 27, 2016\n",
      "\n",
      "12 miles (total) / 2 hours (total) = 6 miles per hour.\n",
      "\n",
      "👍\n",
      "\n",
      "👎\n",
      "\n",
      "ℹ️\n",
      "\n",
      "🚩\n",
      "\n",
      "Ms. Sue\n",
      "\n",
      "Aug 27, 2016\n",
      "\n",
      "3 miles per hour\n",
      "\n",
      "👍\n",
      "\n",
      "👎\n",
      "\n",
      "ℹ️\n",
      "\n",
      "🚩\n",
      "\n",
      "Ms. Sue\n",
      "\n",
      "Aug 27, 2016\n",
      "\n",
      "3 miles per hour\n",
      "\n",
      "👍\n",
      "\n",
      "👎\n",
      "\n",
      "ℹ️\n",
      "\n",
      "🚩\n",
      "\n",
      "Ms. Sue\n",
      "\n",
      "Aug 27, 2016\n",
      "\n",
      "3 miles per hour\n",
      "\n",
      "👍\n",
      "\n",
      "👎\n",
      "\n",
      "ℹ️\n",
      "\n",
      "🚩\n",
      "\n",
      "Ms. Sue\n",
      "\n",
      "Aug 27, 2016\n",
      "\n",
      "3 miles per hour, 4 miles per hour\n",
      "\n",
      "👍\n",
      "\n",
      "👎\n",
      "\n",
      "ℹ️\n",
      "\n",
      "🚩\n",
      "\n",
      "Ms. Sue\n",
      "\n",
      "Aug 27, 2016\n",
      "\n",
      "3 miles per hour, 4 miles per hour\n",
      "\n",
      "👍\n",
      "\n",
      "👎\n",
      "\n",
      "ℹ️\n",
      "\n",
      "🚩\n",
      "\n",
      "Ms. Sue\n",
      "\n",
      "Aug 27, 2016\n",
      "\n",
      "3 miles per hour, 4 miles per hour\n",
      "\n",
      "👍\n",
      "\n",
      "👎\n",
      "\n",
      "ℹ️\n",
      "\n",
      "🚩\n",
      "\n",
      "Ms. Sue\n",
      "\n",
      "Aug 27, 2016\n",
      "\n",
      "3 miles per hour, 4 miles per hour\n",
      "\n",
      "👍\n",
      "\n",
      "👎\n",
      "\n",
      "ℹ️\n",
      "\n",
      "🚩\n",
      "\n",
      "Ms. Sue\n",
      "\n",
      "Aug 27, 2016\n",
      "\n",
      "3 miles per hour, 4 miles per hour\n",
      "\n",
      "👍\n",
      "\n",
      "👎\n",
      "\n",
      "ℹ️\n",
      "\n",
      "🚩\n",
      "\n",
      "Ms\n",
      "====================\n",
      "SFT:\n",
      " \n",
      "\n",
      "/\n",
      "\n",
      " How many miles does Marissa walk in the first 2 hours? ** Marissa walks 4+2=<<4+2=6>>6 miles in the first 2 hours.\n",
      " How many miles does Marissa have left to walk? ** She has 12-6=<<12-6=6>>6 miles left to walk.\n",
      " How many hours does Marissa spend walking the remaining 6 miles? ** She spends 6/4=<<6/4=1.5>>1.5 hours walking the remaining 6 miles.\n",
      " What is Marissa's average speed? ** Her average speed is 4/1.5=<<4/1.5=2>>2MPH\n",
      " #### 2/1.5=2\n",
      "/\n",
      "\n",
      " How many miles does Marissa walk in the first 4 hours? ** Marissa walks 4+2=<<4+2=6>>6 miles in the first 4 hours.\n",
      " How many miles does Marissa have left to walk? ** She has 12-6=<<12-6=6>>6 miles left to walk.\n",
      " How many hours does Marissa spend walking the remaining 6 miles? ** She spends 6/4=<<6/4=1.5>>1.5 hours walking the remaining 6 miles.\n",
      " What is Marissa's average speed? ** Her average speed is 4/1.5=<<4/1.5=2>>2MPH\n",
      " #### 2\n",
      "/\n",
      "\n",
      " How many miles does Marissa walk in the first 2 hours? ** Marissa walks 4+2=<<4+2=6>>6 miles in the first 2 hours.\n",
      " How many miles does Marissa have left to walk? ** She has 12-6=<<12-6=6>>6 miles left to walk.\n",
      " How many hours does Marissa spend walking the remaining 6 miles? ** She spends 6/4=<<6/4=1.5>>1.5 hours walking the remaining 6 miles.\n",
      " What is Marissa's average speed? ** Her average speed is 4/1.5=<<4/1.5=2>>2MPH\n",
      " #### 2\n",
      "/\n",
      "\n",
      " How many miles does Marissa walk in the first 4 hours? ** Marissa walks 4+2=\n"
     ]
    }
   ],
   "source": [
    "print(\"BASE:\\n\", output_base[0]['generated_text'])\n",
    "print(\"=\"*20)\n",
    "print(\"SFT:\\n\", output_sft['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f6cd9ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid/s3/opengptx/behzad_shomali/miniforge3/envs/HF/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.88s/it]\n",
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n",
      "The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/Behzadshomali/Teuken3.7B_IT_LoRA/commit/008e5b23f1bbcaa046bfe2d848fa435539cb11e7', commit_message='Upload tokenizer', commit_description='', oid='008e5b23f1bbcaa046bfe2d848fa435539cb11e7', pr_url=None, repo_url=RepoUrl('https://huggingface.co/Behzadshomali/Teuken3.7B_IT_LoRA', endpoint='https://huggingface.co', repo_type='model', repo_id='Behzadshomali/Teuken3.7B_IT_LoRA'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3,4\"\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "from trl import clone_chat_template\n",
    "\n",
    "# Base model (same one you used during training)\n",
    "base_model = \"Behzadshomali/Teuken3.7B\"\n",
    "\n",
    "# Your LoRA model (local folder or HF Hub repo)\n",
    "lora_model = \"/raid/s3/opengptx/behzad_shomali/instruction_tuning/Teuken3.73T_IT_GSM8K_socratic/17_22_56/checkpoint-675/\"\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "\n",
    "# Load base model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "model, tokenizer, _ = clone_chat_template(model, tokenizer, \"Qwen/Qwen3-0.6B\")\n",
    "\n",
    "# Load LoRA on top of base\n",
    "model = PeftModel.from_pretrained(model, lora_model)\n",
    "\n",
    "# Merge LoRA adapters into the base model\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "\n",
    "revision = \"22_04_11_checkpoint-1125\"\n",
    "# model.push_to_hub(\n",
    "#     \"Behzadshomali/Teuken3.7B_IT_LoRA\",\n",
    "#     revision=revision,\n",
    "#     private=True,             # optional\n",
    "#     use_temp_dir=True,        # makes sure full files are synced\n",
    "#     commit_message=\"Add merged LoRA model with custom code\",\n",
    "#     safe_serialization=True\n",
    "# )\n",
    "\n",
    "tokenizer.push_to_hub(\"Behzadshomali/Teuken3.7B_IT_LoRA\", revision=revision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad136ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "The model 'GPT2ForCausalLM' is not supported for text-generation. Supported models are ['PeftModelForCausalLM', 'ArceeForCausalLM', 'AriaTextForCausalLM', 'BambaForCausalLM', 'BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BitNetForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'Cohere2ForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'DeepseekV2ForCausalLM', 'DeepseekV3ForCausalLM', 'DiffLlamaForCausalLM', 'DogeForCausalLM', 'Dots1ForCausalLM', 'ElectraForCausalLM', 'Emu3ForCausalLM', 'ErnieForCausalLM', 'Ernie4_5ForCausalLM', 'Ernie4_5_MoeForCausalLM', 'Exaone4ForCausalLM', 'FalconForCausalLM', 'FalconH1ForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'Gemma3ForConditionalGeneration', 'Gemma3ForCausalLM', 'Gemma3nForConditionalGeneration', 'Gemma3nForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'Glm4ForCausalLM', 'Glm4MoeForCausalLM', 'GotOcr2ForConditionalGeneration', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GptOssForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'GraniteMoeHybridForCausalLM', 'GraniteMoeSharedForCausalLM', 'HeliumForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'Lfm2ForCausalLM', 'LlamaForCausalLM', 'Llama4ForCausalLM', 'Llama4ForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MiniMaxForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'ModernBertDecoderForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'Olmo2ForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'Phi4MultimodalForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen3ForCausalLM', 'Qwen3MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'SmolLM3ForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'xLSTMForCausalLM', 'XmodForCausalLM', 'ZambaForCausalLM', 'Zamba2ForCausalLM'].\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4,5\"\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "generator = pipeline(\n",
    "    \"text-generation\", \n",
    "    model, \n",
    "    tokenizer=tokenizer, \n",
    "    device_map=\"auto\",\n",
    "    temperature=.3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ff9b78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "question2 = \"Marissa is hiking a 12-mile trail. She took 1 hour to walk the first 4 miles, then another hour to walk the next two miles. If she wants her average speed to be 4 miles per hour, what speed (in miles per hour) does she need to walk the remaining distance?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c737f2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = generator([{\"role\": \"user\", \"content\": question2}], max_new_tokens=512, return_full_text=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0720f14f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- The total time she spent walking the first 4 miles was 1+1 = <<1+1=2>>2 hours\n",
      "- The total time she spent walking the next two miles was 2+1 = <<2+1=3>>3 hours\n",
      "- The total time she spent walking the entire trail was 2+3 = <<2+3=5>>5 hours\n",
      "- The total distance she walked is 12 miles\n",
      "- Her average speed is 4 miles per hour\n",
      "- She needs to walk the remaining distance at 4+5 = <<4+5=9>>9 miles per hour\n",
      " #### 9.00000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "print(output['generated_text'][1]['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4a4839",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid/s3/opengptx/behzad_shomali/miniforge3/envs/HF/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "import os\n",
    "from pathlib import Path\n",
    "from transformers import TrainerCallback\n",
    "import json\n",
    "from datetime import datetime\n",
    "import asyncio\n",
    "import threading\n",
    "import wandb\n",
    "import process\n",
    "def lighteval_async(checkpoint_path, cuda_devices=\"3,4\"):\n",
    "    output_dir = \"/raid/s3/opengptx/behzad_shomali/evaluation_results/sft_intermediate_results/\"\n",
    "    multi_gpu_command = \"--multi_gpu\" if len(cuda_devices.split(',')) > 1 else \"\"\n",
    "\n",
    "    command = f\"\"\"\\\n",
    "CUDA_VISIBLE_DEVICES={cuda_devices} accelerate launch \\\n",
    "    {multi_gpu_command} \\\n",
    "    --main_process_port 2000 \\\n",
    "    --num_processes {len(cuda_devices.split(','))} \\\n",
    "    --num_machines 1 \\\n",
    "    -m \\\n",
    "    lighteval accelerate \\\n",
    "    \"model_name={checkpoint_path},trust_remote_code=True,use_chat_template=True\" \\\n",
    "    \"leaderboard|gsm8k|0|0,leaderboard|hellaswag|0|0\" \\\n",
    "    \"--max_samples 100\" \\\n",
    "    --output-dir {output_dir} \\\n",
    "\"\"\"\n",
    "    \n",
    "    # # Run subprocess asynchronously\n",
    "    # proc = asyncio.create_subprocess_shell(\n",
    "    #     command,\n",
    "    #     stdout=asyncio.subprocess.PIPE,\n",
    "    #     stderr=asyncio.subprocess.PIPE,\n",
    "    #     cwd=os.getcwd()\n",
    "    # )\n",
    "\n",
    "    # stdout, stderr = proc.communicate()\n",
    "\n",
    "    # if proc.returncode != 0:\n",
    "    #     print(\"Evaluation failed!\", stderr.decode())\n",
    "    #     return None\n",
    "\n",
    "    # # Find latest results JSON\n",
    "    # output_dir = Path(output_dir)\n",
    "    # json_files = [f for f in output_dir.glob(\"*.json\") if f.is_file()]\n",
    "\n",
    "    # if not json_files:\n",
    "    #     return None\n",
    "\n",
    "    # json_files.sort(key=lambda f: f.stat().st_ctime, reverse=True)\n",
    "    # results_file = json_files[0]\n",
    "\n",
    "    # with open(results_file, \"r\") as f:\n",
    "    #     results_dict = json.load(f)[\"results\"]\n",
    "\n",
    "    # final_results = {\n",
    "    #     benchmark: v[\"qem\"]\n",
    "    #     for benchmark, v in results_dict.items()\n",
    "    #     if benchmark != \"all\"\n",
    "    # }\n",
    "\n",
    "    # return final_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "138c94c6",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'coroutine' object has no attribute 'communicate'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mlighteval_async\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mX\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 36\u001b[39m, in \u001b[36mlighteval_async\u001b[39m\u001b[34m(checkpoint_path, cuda_devices)\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# Run subprocess asynchronously\u001b[39;00m\n\u001b[32m     29\u001b[39m proc = asyncio.create_subprocess_shell(\n\u001b[32m     30\u001b[39m     command,\n\u001b[32m     31\u001b[39m     stdout=asyncio.subprocess.PIPE,\n\u001b[32m     32\u001b[39m     stderr=asyncio.subprocess.PIPE,\n\u001b[32m     33\u001b[39m     cwd=os.getcwd()\n\u001b[32m     34\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m stdout, stderr = \u001b[43mproc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcommunicate\u001b[49m()\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m proc.returncode != \u001b[32m0\u001b[39m:\n\u001b[32m     39\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mEvaluation failed!\u001b[39m\u001b[33m\"\u001b[39m, stderr.decode())\n",
      "\u001b[31mAttributeError\u001b[39m: 'coroutine' object has no attribute 'communicate'"
     ]
    }
   ],
   "source": [
    "lighteval_async(\"X\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
