{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "229057db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db52941c",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_files = {\n",
    "    \"OpenMathInstruct-2 (5274)\": \"/raid/s3/opengptx/behzad_shomali/evaluation_results/teuken3.7B_IT_LoRA-OpenMathInstruct-2/2025_08_26-09_59_53/checkpoint-5274/results/Behzadshomali/Teuken3.7B_IT_LoRA-OpenMathInstruct-2/results_2025-08-27T02-06-25.078549.json\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f08887",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_result(\n",
    "    result_file,\n",
    "    ax,\n",
    "    benchmarks_dict,\n",
    "    legend=None,\n",
    "    hatch=\"\"\n",
    "):\n",
    "    results_json = json.load(open(result_file, \"r\"))\n",
    "    labels, results, results_stderr = [], [], []\n",
    "    for benchmark, metric in benchmarks_dict.items():\n",
    "        labels.append(benchmark)\n",
    "        results.append(results_json[\"results\"][\"benchmark\"][metric])\n",
    "        results_stderr.append(results_json[\"results\"][\"benchmark\"][f\"{metric}_stderr\"])\n",
    "\n",
    "    if legend is not None:\n",
    "        ax.barh(labels, results, label=legend, alpha=0.5, hatch=hatch)\n",
    "    else:\n",
    "        ax.barh(labels, results, alpha=0.5, hatch=hatch)\n",
    "\n",
    "    ax.errorbar(\n",
    "        results,\n",
    "        labels,\n",
    "        xerr=results_stderr,\n",
    "        fmt='o'\n",
    "    )\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c83670be",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmarks = {\n",
    "    \"leaderboard:arc:challenge:0\": \"acc_norm\",\n",
    "    \"leaderboard:arc:challenge:5\": \"acc_norm\",\n",
    "    \"leaderboard:arc:gsm8k:0\": \"qem\",\n",
    "    \"leaderboard:arc:gsm8k:5\": \"qem\",\n",
    "    \"leaderboard:hellaswag:0\": \"acc_norm\",\n",
    "    \"leaderboard:hellaswag:5\": \"acc_norm\",\n",
    "    \"leaderboard:mmlu:high_school_mathematics:0\": \"acc\",\n",
    "    \"leaderboard:mmlu:high_school_mathematics:5\": \"acc\",\n",
    "    \"leaderboard:truthfulqa:mc:0\": \"truthfulqa_mc2\",\n",
    "    \"leaderboard:truthfulqa:mc:5\": \"truthfulqa_mc2\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d2c4fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6da8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for legend, result_file in result_files.items():\n",
    "    if index != 1:\n",
    "        ax, sorted_benchmarks, labels = plot_result(result_file, ax, sorted_benchmarks=sorted_benchmarks, labels=labels, legend=legend, k=index, benchmark_group=\"leaderboard|hellaswag\", metric=\"acc\")\n",
    "    index += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
